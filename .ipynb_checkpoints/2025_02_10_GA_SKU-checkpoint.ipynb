{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8736c04-ece5-42fe-a9df-e99b905ed0f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6e856c-6780-46da-af11-abe7d90f8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from pyomo.environ import *\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da370d04-c838-4f6d-b92f-dbd9594ae0d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24040d8-1567-4a5d-8da1-7d302c0d4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Matteo.Gabellini\\OneDrive - Alma Mater Studiorum Università di Bologna\\DOTTORATO\\1.RICERCA\\0.CONFERENCE PAPER\\6.ICIL\\1.WAREHOUSE ALLOCATION\\0.DATA\\DatasetClean.csv')\n",
    "df['Articolo'] = df['Articolo'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e0fef-c908-422a-a6c1-aa370f398638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol = df.groupby('Articolo')[['Volume pezzo']].median().reset_index()\n",
    "df_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa12b51-0333-46ad-839f-681295f316b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = pd.read_excel(r'C:\\Users\\Matteo.Gabellini\\OneDrive - Alma Mater Studiorum Università di Bologna\\DOTTORATO\\1.RICERCA\\0.CONFERENCE PAPER\\6.ICIL\\1.WAREHOUSE ALLOCATION\\0.DATA\\Giacenza media articoli 2024.xlsx')\n",
    "df_stock['ARTICOLO'] = df_stock['ARTICOLO'].astype(str)\n",
    "df_stock = df_stock.groupby('ARTICOLO')[['GIACENZA MEDIA']].mean().reset_index()\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2270edb-8e20-45e9-bbe4-68576ef3f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the join based on 'Articolo'\n",
    "df_stock_vol = df_stock.merge(df_vol[['Articolo','Volume pezzo']], how='left', left_on='ARTICOLO', right_on='Articolo')\n",
    "\n",
    "# Drop duplicate column 'ARTICOLO' after merge\n",
    "df_stock_vol.drop(columns=['Articolo'], inplace=True)\n",
    "\n",
    "#Compute stock in volum\n",
    "df_stock_vol['Volume pezzo [m3]'] = df_stock_vol['Volume pezzo'] / 1000\n",
    "df_stock_vol['Giacenza Pezzi Volume [m3]'] = df_stock_vol['GIACENZA MEDIA'] * df_stock_vol['Volume pezzo [m3]']\n",
    "\n",
    "#df_stock_vol = df_stock_vol.drop_duplicates()\n",
    "\n",
    "df_stock_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed94a0-2529-4a99-8db2-66e80f5f6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_vol['Giacenza Pezzi Volume [m3]'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13d91f-89b1-4366-ad5f-562e182b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join to maintain the original number of rows in df\n",
    "df = df.merge(df_stock_vol[['ARTICOLO', 'Giacenza Pezzi Volume [m3]']], how='left', left_on='Articolo', right_on='ARTICOLO')\n",
    "\n",
    "df['Volume evaso [m3]'] = df['Pezzi evasi'] * df['Volume pezzo'] / 1000\n",
    "\n",
    "# Drop the extra 'ARTICOLO' column from df_stock_vol (after the merge)\n",
    "df.drop(columns=['ARTICOLO'], inplace=True)\n",
    "\n",
    "# Ensure no additional duplicates were introduced\n",
    "#df = df.drop_duplicates()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d87d2a-21f2-4291-aeff-c80d8072a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = 'Articolo').groupby('Articolo')['Giacenza Pezzi Volume [m3]'].sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d3002-d3e6-4028-aaec-2c05fde3846b",
   "metadata": {},
   "source": [
    "# DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f808202-6714-496d-90b7-172d98b984c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_population(gene_space, num_genes, sol_per_pop):\n",
    "    \"\"\"\n",
    "    Generate an initial random population.\n",
    "    \n",
    "    Each individual is a vector of length `num_genes`, where each gene is chosen from `gene_space`.\n",
    "    \n",
    "    Parameters:\n",
    "        gene_space (list or array): Possible gene values.\n",
    "        num_genes (int): Number of genes per individual.\n",
    "        sol_per_pop (int): Number of individuals (solutions) in the population.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Population array of shape (sol_per_pop, num_genes).\n",
    "    \"\"\"\n",
    "    return np.random.choice(gene_space, size=(sol_per_pop, num_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da673fa3-fd82-4e01-8259-3796f29594e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_space = [0,1]\n",
    "num_genes = len(df['Articolo'].dropna().unique())\n",
    "sol_per_pop = 4\n",
    "population = initial_population(gene_space = gene_space, num_genes=num_genes, sol_per_pop=sol_per_pop)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea678e00-ec4b-417d-8680-9933197123cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fitness_unfitness_func(population, stock, capacity_A, capacity_B, df, penalty_factor,fitness_type = 'BASE'):\n",
    "    \"\"\"\n",
    "    Compute fitness and unfitness for each individual in the population.\n",
    "    \n",
    "    **Fitness Calculation:**\n",
    "      - For each individual, create an allocation DataFrame mapping unique 'Articolo' values\n",
    "        to the individual's warehouse assignment.\n",
    "      - Merge this allocation with the input DataFrame (which must contain columns 'Articolo'\n",
    "        and 'Num. Ordine').\n",
    "      - For each order, compute the number of transitions between warehouses (from 1 to 0 and 0 to 1).\n",
    "      - The fitness is the sum of all transitions.\n",
    "    \n",
    "    **Unfitness Calculation:**\n",
    "      - Compute the total stock assigned to Warehouse A (assignment 1) and Warehouse B (assignment 0).\n",
    "      - If a warehouse's capacity is exceeded, add a penalty proportional to the excess.\n",
    "    \n",
    "    Parameters:\n",
    "        population (np.ndarray): Array of individuals (shape: sol_per_pop x num_genes).\n",
    "        stock (np.ndarray): Array of stock values associated with each 'Articolo'.\n",
    "        capacity_A (float): Capacity for Warehouse A.\n",
    "        capacity_B (float): Capacity for Warehouse B.\n",
    "        df (pd.DataFrame): DataFrame containing order information; must include 'Articolo' and 'Num. Ordine'.\n",
    "        penalty_factor (float): Penalty multiplier for capacity violations.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Two NumPy arrays: (population_fitness, population_unfitness)\n",
    "    \"\"\"\n",
    "    fitness_list = []\n",
    "    unfitness_list = []\n",
    "    unique_categories = df['Articolo'].dropna().unique()\n",
    "    \n",
    "    for individual in population:\n",
    "        # Map each unique article to its warehouse assignment from the individual.\n",
    "        df_allocation = pd.DataFrame({'Articolo': unique_categories, 'Warehouse': individual})\n",
    "        # Merge the allocation with the main DataFrame.\n",
    "        df_merged = pd.merge(df, df_allocation, on='Articolo', how='left')\n",
    "        df_merged['Warehouse'] = df_merged['Warehouse'].replace({0: 'A', 1: 'B'})\n",
    "\n",
    "        \n",
    "        if fitness_type == 'BASE':\n",
    "\n",
    "            #Route analysis\n",
    "            route_grouped_df = df_merged.groupby(['Mese-Giorno','Percorso']).agg({\n",
    "                'Warehouse': lambda x: list(x.unique()),  # Stores unique warehouses as lists\n",
    "                'Volume evaso [m3]': 'sum'  # Sums up volume\n",
    "            }).reset_index()\n",
    "        \n",
    "            route_vol_B = route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\") & ~route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\")]['Volume evaso [m3]'].sum() \n",
    "        \n",
    "            # Step 1: Extract valid routes containing both 'A' and 'B' in 'Warehouse'\n",
    "            AB_order_list = route_grouped_df[\n",
    "                route_grouped_df['Warehouse'].astype(str).str.contains(r'A') & \n",
    "                route_grouped_df['Warehouse'].astype(str).str.contains(r'B')\n",
    "            ][['Mese-Giorno', 'Percorso']].apply(tuple, axis=1).tolist()\n",
    "            \n",
    "            # Step 2: Compute total volume for each route\n",
    "            df_AB_route_volume = df_merged.groupby(['Mese-Giorno', 'Percorso', 'Warehouse'])[['Volume evaso [m3]']].sum().reset_index()\n",
    "            \n",
    "            # Step 3: Filter only the relevant routes and sum by Warehouse\n",
    "            df_AB_route_volume = df_AB_route_volume[\n",
    "                df_AB_route_volume[['Mese-Giorno', 'Percorso']].apply(tuple, axis=1).isin(AB_order_list)\n",
    "            ].groupby('Warehouse')['Volume evaso [m3]'].sum()\n",
    "            \n",
    "        \n",
    "            #compute stock volume in A and in B\n",
    "            stock_A = df_merged.drop_duplicates(subset=['Articolo']).groupby('Warehouse')['Giacenza Pezzi Volume [m3]'].sum().get('A', 0)\n",
    "            stock_B = df_merged.drop_duplicates(subset=['Articolo'])['Giacenza Pezzi Volume [m3]'].sum() -  stock_A\n",
    "                \n",
    "            individual_fitness = ((stock_B + df_AB_route_volume.get('B', 0) + route_vol_B + df_AB_route_volume.get('B', 0) + route_vol_B)*0.33)/(220*0.77*10*0.5)\n",
    "\n",
    "        else:\n",
    "\n",
    "            individual_fitness = 0\n",
    "            \n",
    "        fitness_list.append(individual_fitness)\n",
    "        \n",
    "        # Calculate total stock for each warehouse.\n",
    "        total_A = np.sum(individual * stock)\n",
    "        total_B = np.sum((1 - individual) * stock)\n",
    "        penalty = 0\n",
    "        if total_A > capacity_A:\n",
    "            penalty += penalty_factor * (total_A - capacity_A)\n",
    "        if total_B > capacity_B:\n",
    "            penalty += penalty_factor * (total_B - capacity_B)\n",
    "\n",
    "        # # Add penalty if total_B is lower than half of capacity_B\n",
    "        #     if total_B < (capacity_B / 2):\n",
    "        #         penalty += penalty_factor * ((capacity_B / 2) - total_B)\n",
    "\n",
    "        unfitness_list.append(penalty)\n",
    "        \n",
    "    return np.array(fitness_list), np.array(unfitness_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1878d2c-5d65-474f-9cdf-a6b095e5867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock = np.ones(num_genes) \n",
    "stock = np.zeros(num_genes)\n",
    "# capacity_A = 0 \n",
    "# capacity_B = 0 \n",
    "capacity_A = 999999999999999\n",
    "capacity_B = 999999999999999\n",
    "penalty_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a187f-f955-4b63-b754-281f5c513bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_fitness, population_unfitness = fitness_unfitness_func(population, stock, capacity_A, capacity_B, df, penalty_factor)\n",
    "population_fitness, population_unfitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e136dfd-97e5-4a87-8bd0-b4d297c0cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PARENT SELECTION CRITERIA\n",
    "def ParentSelection(population, sol_per_pop, population_fitness, population_unfitness):\n",
    "    \"\"\"\n",
    "    Select two parents using tournament selection.\n",
    "    \n",
    "    Four unique individuals are chosen at random. Among the first two candidates, the one with\n",
    "    lower fitness is chosen as the first parent. Similarly, among the next two, the one with lower\n",
    "    fitness is chosen as the second parent.\n",
    "    \n",
    "    Parameters:\n",
    "        population (np.ndarray): Population array.\n",
    "        sol_per_pop (int): Total number of individuals in the population.\n",
    "        population_fitness (np.ndarray): Fitness values for the population.\n",
    "        population_unfitness (np.ndarray): Unfitness values for the population (not used here).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Two parents (each a 1D numpy array).\n",
    "    \"\"\"\n",
    "    random_indices = np.random.choice(np.arange(sol_per_pop), 4, replace=False)\n",
    "    # print('random_indices',random_indices)\n",
    "    \n",
    "    # Compare first two candidates.\n",
    "    if population_fitness[random_indices[0]] >= population_fitness[random_indices[1]]:\n",
    "        first_parent = population[random_indices[1]]\n",
    "    else:\n",
    "        first_parent = population[random_indices[0]]\n",
    "    \n",
    "    # Compare the next two candidates.\n",
    "    if population_fitness[random_indices[2]] >= population_fitness[random_indices[3]]:\n",
    "        second_parent = population[random_indices[2]]\n",
    "    else:\n",
    "        second_parent = population[random_indices[3]]\n",
    "    \n",
    "    return first_parent, second_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839d805-88c3-40cc-ba80-66f52291832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent1, parent2 = ParentSelection(population, sol_per_pop, population_fitness, population_unfitness)\n",
    "parent1, parent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525fd22-1d1b-4cb6-b240-eeb9eb8ea460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def child_generation(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Generate a child individual via one-point crossover followed by a mutation.\n",
    "    \n",
    "    **Crossover:** A random crossover index is selected, and the child's genes are taken \n",
    "    from parent1 up to this index and from parent2 thereafter.\n",
    "    \n",
    "    **Mutation:** Two random positions in the child's gene sequence are swapped.\n",
    "    \n",
    "    Parameters:\n",
    "        parent1 (np.ndarray): First parent's gene sequence.\n",
    "        parent2 (np.ndarray): Second parent's gene sequence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The generated child individual.\n",
    "    \"\"\"\n",
    "    child = parent2.copy()\n",
    "    crossover_index = random.randint(1, len(parent1) - 1)\n",
    "    child[:crossover_index] = parent1[:crossover_index]\n",
    "    \n",
    "    # Mutation: Swap two random indices.\n",
    "    mutation_indices = np.random.choice(len(child), size=2, replace=False)\n",
    "    child[mutation_indices[0]], child[mutation_indices[1]] = child[mutation_indices[1]], child[mutation_indices[0]]\n",
    "    \n",
    "    return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40468512-50da-48bb-881f-eff34db37189",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d56e2-48e7-4ec3-bc8a-d29a285ac5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636c8a1-a608-48cc-8aa5-b8cb927599b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "child = child_generation(parent1 = parent1, parent2=parent2)\n",
    "child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaf396-cde4-4226-ab68-ae485e3b62a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.COMPUTE FITNESS AND UNFITNESS\n",
    "def update_fitness_unfitness_func(population, df, stock, population_fitness, population_unfitness, child, worse_individual_index,fitness_type = 'BASE'):\n",
    "    \n",
    "\n",
    "    #CHILD IMPROVEMENT\n",
    "\n",
    "    \n",
    "\n",
    "    #FITNESS FUNCTION\n",
    "\n",
    "\n",
    "    unique_categories = df['Articolo'].dropna().unique()\n",
    "    df_allocation = pd.DataFrame({'Articolo': unique_categories, 'Warehouse': child})\n",
    "    # Merge the allocation with the main DataFrame.\n",
    "    df_merged = pd.merge(df, df_allocation, on='Articolo', how='left')\n",
    "    df_merged['Warehouse'] = df_merged['Warehouse'].replace({0: 'A', 1: 'B'})\n",
    "\n",
    "    \n",
    "    if fitness_type == 'BASE':\n",
    "\n",
    "        #Route analysis\n",
    "        route_grouped_df = df_merged.groupby(['Mese-Giorno','Percorso']).agg({\n",
    "            'Warehouse': lambda x: list(x.unique()),  # Stores unique warehouses as lists\n",
    "            'Volume evaso [m3]': 'sum'  # Sums up volume\n",
    "        }).reset_index()\n",
    "    \n",
    "        route_vol_B = route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\") & ~route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\")]['Volume evaso [m3]'].sum() \n",
    "    \n",
    "        # Step 1: Extract valid routes containing both 'A' and 'B' in 'Warehouse'\n",
    "        AB_order_list = route_grouped_df[\n",
    "            route_grouped_df['Warehouse'].astype(str).str.contains(r'A') & \n",
    "            route_grouped_df['Warehouse'].astype(str).str.contains(r'B')\n",
    "        ][['Mese-Giorno', 'Percorso']].apply(tuple, axis=1).tolist()\n",
    "        \n",
    "        # Step 2: Compute total volume for each route\n",
    "        df_AB_route_volume = df_merged.groupby(['Mese-Giorno', 'Percorso', 'Warehouse'])[['Volume evaso [m3]']].sum().reset_index()\n",
    "        \n",
    "        # Step 3: Filter only the relevant routes and sum by Warehouse\n",
    "        df_AB_route_volume = df_AB_route_volume[\n",
    "            df_AB_route_volume[['Mese-Giorno', 'Percorso']].apply(tuple, axis=1).isin(AB_order_list)\n",
    "        ].groupby('Warehouse')['Volume evaso [m3]'].sum()\n",
    "        \n",
    "    \n",
    "        #compute stock volume in A and in B\n",
    "        stock_A = df_merged.drop_duplicates(subset=['Articolo']).groupby('Warehouse')['Giacenza Pezzi Volume [m3]'].sum().get('A', 0)\n",
    "        stock_B = df_merged.drop_duplicates(subset=['Articolo'])['Giacenza Pezzi Volume [m3]'].sum() -  stock_A\n",
    "            \n",
    "        new_individual_fitness = ((stock_B + df_AB_route_volume.get('B', 0) + route_vol_B + df_AB_route_volume.get('B', 0) + route_vol_B)*0.33)/(220*0.77*10*0.5)\n",
    "\n",
    "    else:\n",
    "    \n",
    "        new_individual_fitness = 0\n",
    "    \n",
    "    population_fitness[worse_individual_index] = new_individual_fitness.reshape(1)\n",
    "\n",
    "    #UNFITNESS FUNCTION\n",
    "\n",
    "    total_A = np.sum(child * stock)\n",
    "    total_B = np.sum((1 - child) * stock)\n",
    "    penalty = 0\n",
    "    if total_A > capacity_A:\n",
    "        penalty += penalty_factor * (total_A - capacity_A)\n",
    "    if total_B > capacity_B:\n",
    "        penalty += penalty_factor * (total_B - capacity_B)\n",
    "\n",
    "    # # Add penalty if total_B is lower than half of capacity_B\n",
    "    # if total_B < (capacity_B / 2):\n",
    "    #     penalty += penalty_factor * ((capacity_B / 2) - total_B)\n",
    "\n",
    "    new_individual_unfitness = penalty\n",
    "\n",
    "    population_unfitness[worse_individual_index] = new_individual_unfitness\n",
    "\n",
    "    return population_fitness, population_unfitness, new_individual_fitness, new_individual_unfitness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f38c6-203f-4847-acd5-177216f800d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def new_population(population, child, stock, capacity_A, capacity_B, population_fitness, population_unfitness, df, penalty_factor):\n",
    "    \"\"\"\n",
    "    Update the population by replacing the worst individual with the new child.\n",
    "    \n",
    "    If all individuals have zero unfitness, the worst is determined by the highest fitness;\n",
    "    otherwise, the worst is determined by the highest unfitness.\n",
    "    \n",
    "    After replacement, the fitness and unfitness of the entire population are re-calculated.\n",
    "    \n",
    "    Parameters:\n",
    "        population (np.ndarray): Current population.\n",
    "        child (np.ndarray): New child individual.\n",
    "        stock (np.ndarray): Stock values.\n",
    "        capacity_A (float): Capacity for Warehouse A.\n",
    "        capacity_B (float): Capacity for Warehouse B.\n",
    "        population_fitness (np.ndarray): Fitness values for the population.\n",
    "        population_unfitness (np.ndarray): Unfitness values for the population.\n",
    "        df (pd.DataFrame): DataFrame containing order information.\n",
    "        penalty_factor (float): Penalty multiplier.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated (population, population_fitness, population_unfitness,\n",
    "                best_individual, best_individual_fitness, best_individual_unfitness).\n",
    "    \"\"\"\n",
    "    if np.all(population_unfitness == 0):\n",
    "        worse_individual_index = np.argmax(population_fitness)\n",
    "    else:\n",
    "        worse_individual_index = np.argmax(population_unfitness)\n",
    "    \n",
    "    population[worse_individual_index] = child\n",
    "\n",
    "    population_fitness, population_unfitness, new_individual_fitness, new_individual_unfitness  = update_fitness_unfitness_func(population, df, stock, population_fitness, population_unfitness, child, worse_individual_index,fitness_type = 'BASE')\n",
    "\n",
    "    \n",
    "    best_idx = np.argmin(population_fitness)\n",
    "    best_individual = population[best_idx]\n",
    "    best_individual_fitness = population_fitness[best_idx]\n",
    "    best_individual_unfitness = population_unfitness[best_idx]\n",
    "    \n",
    "    return population, population_fitness, population_unfitness, new_individual_fitness, new_individual_unfitness, best_individual, best_individual_fitness, best_individual_unfitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274990-8520-4b94-8bcf-c5b151cf17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "population, population_fitness, population_unfitness, new_individual_fitness, new_individual_unfitness, best_individual, best_individual_fitness, best_individual_unfitness = new_population(population, child, stock, capacity_A, capacity_B, population_fitness, population_unfitness, df, penalty_factor)\n",
    "population, population_fitness, population_unfitness, new_individual_fitness, new_individual_unfitness, best_individual, best_individual_fitness, best_individual_unfitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b30dec-316b-4f4a-9820-1cc1d72307ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. MAIN GENETIC ALGORITHM LOOP WITH PLOTTING EVERY MINUTE ON DUAL Y-AXES\n",
    "def main(day, gene_space, num_genes, sol_per_pop, df, stock, capacity_A, capacity_B, M, penalty_factor):\n",
    "    non_improving_iteration = 0\n",
    "    iteration = 0\n",
    "    child_list = []\n",
    "    start_time = time.time()\n",
    "    last_plot_time = start_time  # Initialize last plot time\n",
    "    \n",
    "    # History lists.\n",
    "    fitness_history = []\n",
    "    unfitness_history = []\n",
    "    iterations = []\n",
    "    A_code_history = []\n",
    "    B_code_history = []\n",
    "    \n",
    "    # INITIAL POPULATION\n",
    "    population = initial_population(gene_space, num_genes, sol_per_pop)\n",
    "    population_fitness, population_unfitness = fitness_unfitness_func(population, stock, capacity_A, capacity_B, df, penalty_factor)\n",
    "    \n",
    "    while non_improving_iteration < M and (time.time() - start_time) < 8 *60 * 60:\n",
    "        iteration += 1\n",
    "        \n",
    "        # PARENT SELECTION.\n",
    "        first_parent, second_parent = ParentSelection(population, sol_per_pop, population_fitness, population_unfitness)\n",
    "        \n",
    "        # CHILD GENERATION.\n",
    "        child = child_generation(first_parent, second_parent)\n",
    "        \n",
    "        # POPULATION UPDATE.\n",
    "        (population, population_fitness, population_unfitness, new_individual_fitness, new_individual_unfitness, best_individual, best_individual_fitness, best_individual_unfitness) = new_population(population, child, stock, capacity_A, capacity_B, population_fitness, population_unfitness, df, penalty_factor)\n",
    "\n",
    "        #VISUALIZE NUMBER OF CODE PER WAREHOUSE \n",
    "        unique_categories = df['Articolo'].dropna().unique()\n",
    "        df_allocation = pd.DataFrame({'Articolo': unique_categories, 'Warehouse': best_individual})\n",
    "        df_merged = pd.merge(df, df_allocation, on='Articolo', how='left')\n",
    "        A_code = list(df_merged.groupby('Warehouse')['Articolo'].nunique())[0]\n",
    "        B_code = df_merged['Articolo'].nunique() - list(df_merged.groupby('Warehouse')['Articolo'].nunique())[0]\n",
    "        \n",
    "        # Record history.\n",
    "        fitness_history.append(best_individual_fitness)\n",
    "        unfitness_history.append(best_individual_unfitness)\n",
    "        iterations.append(iteration)\n",
    "        A_code_history.append(A_code)\n",
    "        B_code_history.append(B_code)\n",
    "\n",
    "\n",
    "        # Plot every 60 seconds on the same figure with two y-axes.\n",
    "        current_time = time.time()\n",
    "        if current_time - last_plot_time >= 10:\n",
    "            # Assume the following lists have been populated during the algorithm:\n",
    "            # iterations, fitness_history, unfitness_history, A_code_history, B_code_history\n",
    "            \n",
    "            fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            # Left subplot: Fitness and Unfitness Evolution\n",
    "            # Plot best fitness on the primary y-axis.\n",
    "            ax_left.plot(iterations, fitness_history, 'o-', color='blue', label='Best Fitness')\n",
    "            ax_left.set_xlabel('Iteration')\n",
    "            ax_left.set_ylabel('Best Fitness', color='blue')\n",
    "            ax_left.tick_params(axis='y', labelcolor='blue')\n",
    "            # Create a twin y-axis to plot best unfitness.\n",
    "            ax_left_right = ax_left.twinx()\n",
    "            ax_left_right.plot(iterations, unfitness_history, 'o-', color='red', label='Best Unfitness')\n",
    "            ax_left_right.set_ylabel('Best Unfitness', color='red')\n",
    "            ax_left_right.tick_params(axis='y', labelcolor='red')\n",
    "            ax_left.set_title(\"Evolution of Fitness and Unfitness\")\n",
    "            ax_left.grid(True)\n",
    "            \n",
    "            # Right subplot: Code A and Code B Evolution\n",
    "            # Plot Code A on the primary y-axis.\n",
    "            ax_right.plot(iterations, A_code_history, 'o-', color='green', label='Code A')\n",
    "            ax_right.set_xlabel('Iteration')\n",
    "            ax_right.set_ylabel('Code A', color='green')\n",
    "            ax_right.tick_params(axis='y', labelcolor='green')\n",
    "            # Create a twin y-axis to plot Code B.\n",
    "            ax_right_right = ax_right.twinx()\n",
    "            ax_right_right.plot(iterations, B_code_history, 'o-', color='purple', label='Code B')\n",
    "            ax_right_right.set_ylabel('Code B', color='purple')\n",
    "            ax_right_right.tick_params(axis='y', labelcolor='purple')\n",
    "            ax_right.set_title(\"Evolution of Code A and Code B\")\n",
    "            ax_right.grid(True)\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "            last_plot_time = current_time\n",
    "        \n",
    "        # Check for duplicate child.\n",
    "        if list(child) in child_list:\n",
    "            non_improving_iteration += 1\n",
    "        else:\n",
    "            non_improving_iteration = 0\n",
    "            child_list.append(list(child))\n",
    "    \n",
    "    computational_time = time.time() - start_time\n",
    "    \n",
    "    # Final plot after the loop finishes on dual y-axes.\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(iterations, fitness_history, 'o-', label='Best Fitness', color='blue')\n",
    "    ax2.plot(iterations, unfitness_history, 'o-', label='Best Unfitness', color='red')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Best Fitness', color='blue')\n",
    "    ax2.set_ylabel('Best Unfitness', color='red')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    plt.title(\"Final Evolution of Best Individual's Fitness and Unfitness\")\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return population, population_fitness, population_unfitness, computational_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22280804-f9c7-48d1-b5fe-36df2240aaf0",
   "metadata": {},
   "source": [
    "## ALLOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d81fb-6f15-4875-9b49-2ee74b461293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allocation = pd.DataFrame({'Articolo': df['Articolo'].dropna().unique()})\n",
    "df_allocation['Warehouse'] = best_individual\n",
    "df_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28898a7-667b-4324-9f90-3b2b19d447c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = 'Articolo').groupby('Articolo')['Giacenza Pezzi Volume [m3]'].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977fc39-451e-4a5b-91e0-7d37d89cf094",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_space = [0,1]\n",
    "num_genes = len(df['Articolo'].dropna().unique())\n",
    "sol_per_pop = 50\n",
    "penalty_factor = 1\n",
    "M = 500000\n",
    "\n",
    "stock = df.drop_duplicates(subset = 'Articolo').groupby('Articolo')['Giacenza Pezzi Volume [m3]'].sum().values\n",
    "capacity_A = 25000 * 0.37\n",
    "capacity_B = 6800 * 0.37\n",
    "\n",
    "#stock = np.zeros(num_genes)\n",
    "#capacity_A = 999999999999999\n",
    "#capacity_B = 999999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1269b6d-c04a-4345-9c19-f35187111371",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.sum(),capacity_A,capacity_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32eddb-58e9-4ffa-a03a-9b0ba30e3b0e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the Genetic Algorithm.\n",
    "best_population, best_fitness_values, best_unfitness_values, comp_time = main(\n",
    "    day=None,\n",
    "    gene_space=gene_space,\n",
    "    num_genes=num_genes,\n",
    "    sol_per_pop=sol_per_pop,\n",
    "    df=df,\n",
    "    stock=stock,\n",
    "    capacity_A=capacity_A,\n",
    "    capacity_B=capacity_B,\n",
    "    M=M,\n",
    "    penalty_factor=penalty_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1717fe1-d687-4484-b55a-bd8fe66660d3",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Global Best Individual:\")\n",
    "print(best_individual)\n",
    "print(\"Global Fitness Values:\")\n",
    "print(best_individual_fitness)\n",
    "print(\"Global Unfitness Values:\")\n",
    "print(best_individual_unfitness)\n",
    "#print(\"Total Computational Time (sec):\", comp_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38efa96-40bd-47ef-9fcd-8b780f0f46f0",
   "metadata": {},
   "source": [
    "# STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6605cdb-a11d-4488-a747-ad068d9279ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom column width\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_columns', None)  # or set a number like 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dc490-e885-458b-a92b-71a7f6d64303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allocation = pd.DataFrame({'Articolo': df['Articolo'].dropna().unique()})\n",
    "df_allocation['Warehouse'] = best_individual\n",
    "df_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98f203-711b-4f7a-b118-0922c73f3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_allocation, on='Articolo', how='left')\n",
    "df['Warehouse'] = df['Warehouse'].replace({0: 'A', 1: 'B'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de22242-34f2-4de5-a73f-76cde5bbc1d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        'Magazzino A': [],\n",
    "        'Magazzino B':[],\n",
    "        \n",
    "        'Codici in A':[],\n",
    "        'Codici in B':[],\n",
    "        \n",
    "        'Stock [m3] in A':[],\n",
    "        'Stock [m3] in B':[],\n",
    "        \n",
    "        '% Ordini completati in AB':[],\n",
    "        '% Ordini completati in A':[], \n",
    "        '% Ordini completati in B':[],\n",
    "\n",
    "        'Vol[m3] Ordini completati in A':[], \n",
    "        'Vol[m3] Ordini completati in B':[],\n",
    "        'Vol[m3] Ordini completati in AB':[],\n",
    "        'Vol[m3] Ordini completati in AB (A)':[],\n",
    "        'Vol[m3] Ordini completati in AB (B)':[],\n",
    "        \n",
    "        '% Rotte completate in AB' :[],\n",
    "        '% Rotte completate in A' :[],\n",
    "        '% Rotte completate in B' :[],\n",
    "\n",
    "        'Vol[m3] Rotte completati in A' :[],\n",
    "        'Vol[m3] Rotte completati in B' :[],\n",
    "        'Vol[m3] Rotte completati in AB' :[],\n",
    "        'Vol[m3] Rotte completati in AB (A)' :[],\n",
    "        'Vol[m3] Rotte completati in AB (B)' :[],\n",
    "\n",
    "    }\n",
    ")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49021b0-ee96-45bb-b7c5-a4070601a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_A = df.groupby('Warehouse')['Articolo'].unique().get('A', 0)\n",
    "assignment_B = df.groupby('Warehouse')['Articolo'].unique().get('B', 0)\n",
    "\n",
    "code_A = df.groupby('Warehouse')['Articolo'].nunique().get('A', 0)\n",
    "code_B = df.groupby('Warehouse')['Articolo'].nunique().get('B', 0)\n",
    "# print('Article division', article_division)\n",
    "\n",
    "#Order analysis\n",
    "order_grouped_df = df.groupby(['Mese-Giorno','Num. Ordine']).agg({\n",
    "    'Warehouse': lambda x: list(x.unique()),  # Stores unique warehouses as lists\n",
    "    'Volume evaso [m3]': 'sum'  # Sums up volume\n",
    "}).reset_index()\n",
    "\n",
    "order_movment_A = len(order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & ~order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]) / len(order_grouped_df) * 100\n",
    "order_movment_B = len(order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\") & ~order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\")]) / len(order_grouped_df) * 100\n",
    "order_movment_AB = len(order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]) / len(order_grouped_df) * 100\n",
    "\n",
    "order_vol_A = order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & ~order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]['Volume evaso [m3]'].sum() \n",
    "order_vol_B = order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\") & ~order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\")]['Volume evaso [m3]'].sum() \n",
    "order_vol_AB = order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]['Volume evaso [m3]'].sum()\n",
    "\n",
    "AB_order_list = list(order_grouped_df[order_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & order_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]['Num. Ordine'])\n",
    "df_AB_order_volume = df.groupby(['Num. Ordine','Warehouse'])[['Volume evaso [m3]']].sum().reset_index()\n",
    "df_AB_order_volume = df_AB_order_volume[df_AB_order_volume['Num. Ordine'].isin(AB_order_list)].groupby('Warehouse')['Volume evaso [m3]'].sum()\n",
    "\n",
    "#Route analysis\n",
    "route_grouped_df = df.groupby(['Mese-Giorno','Percorso']).agg({\n",
    "    'Warehouse': lambda x: list(x.unique()),  # Stores unique warehouses as lists\n",
    "    'Volume evaso [m3]': 'sum'  # Sums up volume\n",
    "}).reset_index()\n",
    "\n",
    "route_movment_A = len(route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & ~route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]) / len(route_grouped_df) * 100\n",
    "route_movment_B = len(route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\") & ~route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\")]) / len(route_grouped_df) * 100\n",
    "route_movment_AB = len(route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]) / len(route_grouped_df) * 100\n",
    "\n",
    "route_vol_A = route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & ~route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]['Volume evaso [m3]'].sum() \n",
    "route_vol_B = route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\") & ~route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\")]['Volume evaso [m3]'].sum() \n",
    "route_vol_AB = route_grouped_df[route_grouped_df['Warehouse'].astype(str).str.contains(r\"'A'\") & route_grouped_df['Warehouse'].astype(str).str.contains(r\"'B'\")]['Volume evaso [m3]'].sum()\n",
    "\n",
    "# Step 1: Extract valid routes containing both 'A' and 'B' in 'Warehouse'\n",
    "AB_order_list = route_grouped_df[\n",
    "    route_grouped_df['Warehouse'].astype(str).str.contains(r'A') & \n",
    "    route_grouped_df['Warehouse'].astype(str).str.contains(r'B')\n",
    "][['Mese-Giorno', 'Percorso']].apply(tuple, axis=1).tolist()\n",
    "\n",
    "# Step 2: Compute total volume for each route\n",
    "df_AB_route_volume = df.groupby(['Mese-Giorno', 'Percorso', 'Warehouse'])[['Volume evaso [m3]']].sum().reset_index()\n",
    "\n",
    "# Step 3: Filter only the relevant routes and sum by Warehouse\n",
    "df_AB_route_volume = df_AB_route_volume[\n",
    "    df_AB_route_volume[['Mese-Giorno', 'Percorso']].apply(tuple, axis=1).isin(AB_order_list)\n",
    "].groupby('Warehouse')['Volume evaso [m3]'].sum()\n",
    "\n",
    "\n",
    "weighted_stock = df.groupby(['Articolo','Warehouse',])['Giacenza Pezzi Volume [m3]'].mean() * (df.groupby(['Articolo','Warehouse',])['Pezzi evasi'].sum() / df.groupby(['Articolo'])['Pezzi evasi'].sum())\n",
    "stock_A = weighted_stock.groupby('Warehouse').sum().get('A', 0)\n",
    "stock_B = weighted_stock.groupby('Warehouse').sum().get('B', 0)\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        'Magazzino A': [assignment_A],\n",
    "        'Magazzino B': [assignment_B],\n",
    "        \n",
    "        'Codici in A':[code_A],\n",
    "        'Codici in B':[code_B],\n",
    "        \n",
    "        'Stock [m3] in A':[stock_A],\n",
    "        'Stock [m3] in B':[stock_B],\n",
    "        \n",
    "        '% Ordini completati in AB':[order_movment_AB],\n",
    "        '% Ordini completati in A':[order_movment_A],\n",
    "        '% Ordini completati in B':[order_movment_B],\n",
    "\n",
    "        '% Rotte completate in AB':[route_movment_AB],\n",
    "        '% Rotte completate in A':[route_movment_A],\n",
    "        '% Rotte completate in B':[route_movment_B],\n",
    "\n",
    "        'Vol[m3] Ordini completati in A':[order_vol_A],\n",
    "        'Vol[m3] Ordini completati in B':[order_vol_B],\n",
    "        'Vol[m3] Ordini completati in AB':[order_vol_AB],\n",
    "        'Vol[m3] Ordini completati in AB (A)':[df_AB_order_volume.get('A', 0)],\n",
    "        'Vol[m3] Ordini completati in AB (B)':[df_AB_order_volume.get('B', 0)],\n",
    "\n",
    "\n",
    "        'Vol[m3] Rotte completati in A':[route_vol_A],\n",
    "        'Vol[m3] Rotte completati in B':[route_vol_B],\n",
    "        'Vol[m3] Rotte completati in AB':[route_vol_AB],\n",
    "        'Vol[m3] Rotte completati in AB (A)' :[df_AB_route_volume.get('A', 0)],\n",
    "        'Vol[m3] Rotte completati in AB (B)' :[df_AB_route_volume.get('B', 0)],\n",
    "\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51e800-d945-47b4-9d2b-a1bafab7fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_excel(r'C:\\Users\\Matteo.Gabellini\\OneDrive - Alma Mater Studiorum Università di Bologna\\DOTTORATO\\1.RICERCA\\0.CONFERENCE PAPER\\6.ICIL\\1.WAREHOUSE ALLOCATION\\3.RESULTS\\SKU.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e653bdc-3b96-410b-893b-51fa66e15f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saturazione_navetta = 0.1\n",
    "\n",
    "df_AA = df_results.copy()\n",
    "df_AA['Accettazione'] = 'A'\n",
    "df_AA['Spedizione'] = 'A'\n",
    "df_AA['T acc [h/gg]'] = ((df_AA['Stock [m3] in B'] + df_AA['Vol[m3] Rotte completati in AB (B)'] + df_AA['Vol[m3] Rotte completati in B'])*0.33)/(220*0.77*10*saturazione_navetta)\n",
    "df_AA['T sped [h/gg]'] = ((df_AA['Vol[m3] Rotte completati in AB (B)'] + df_AA['Vol[m3] Rotte completati in B'])*0.33)/(220*0.77*10*saturazione_navetta)\n",
    "df_AA['T tot [h/gg]'] = df_AA['T acc [h/gg]'] + df_AA['T sped [h/gg]']\n",
    "\n",
    "df_ABA = df_results.copy()\n",
    "df_ABA['Accettazione'] = 'A-B'\n",
    "df_ABA['Spedizione'] = 'A'\n",
    "df_ABA['T acc [h/gg]'] = 0\n",
    "df_ABA['T sped [h/gg]'] = ((df_ABA['Vol[m3] Rotte completati in AB (B)'] + df_ABA['Vol[m3] Rotte completati in B'])*0.33)/(220*0.77*10*saturazione_navetta)\n",
    "df_ABA['T tot [h/gg]'] = df_ABA['T acc [h/gg]'] + df_ABA['T sped [h/gg]']\n",
    "\n",
    "df_AAB = df_results.copy()\n",
    "df_AAB['Accettazione'] = 'A'\n",
    "df_AAB['Spedizione'] = 'A-B'\n",
    "df_AAB['T acc [h/gg]'] = ((df_AA['Stock [m3] in B'] + df_AAB['Vol[m3] Rotte completati in AB (B)'] + df_AAB['Vol[m3] Rotte completati in B'])*0.33)/(220*0.77*10*saturazione_navetta)\n",
    "df_AAB['T sped [h/gg]'] = (df_AA['Vol[m3] Rotte completati in AB (B)']*0.33)/(220*0.77*10*saturazione_navetta)\n",
    "df_AAB['T tot [h/gg]'] = df_AAB['T acc [h/gg]'] + df_AAB['T sped [h/gg]']\n",
    "\n",
    "df_ABAB = df_results.copy()\n",
    "df_ABAB['Accettazione'] = 'A-B'\n",
    "df_ABAB['Spedizione'] = 'A-B'\n",
    "df_ABAB['T acc [h/gg]'] = 0\n",
    "df_ABAB['T sped [h/gg]'] =  (df_ABAB['Vol[m3] Rotte completati in AB (B)']*0.33)/(220*0.77*10*saturazione_navetta)\n",
    "df_ABAB['T tot [h/gg]'] = df_ABAB['T acc [h/gg]'] + df_ABAB['T sped [h/gg]']\n",
    "\n",
    "df_all_results = pd.concat([df_AA,df_ABA,df_AAB,df_ABAB])\n",
    "df_all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682457c-6f9b-4784-a0c3-b5dc3d77fbe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7294d3b-9b29-49b9-a1b0-c27edf561ae9",
   "metadata": {},
   "source": [
    "df_all_results.to_excel(r'C:\\Users\\Matteo.Gabellini\\OneDrive - Alma Mater Studiorum Università di Bologna\\DOTTORATO\\2.CONSULENZA\\AZIENDE\\MELONI\\4.RISULTATI\\0.BRUTE FORCE\\Unibo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39115a60-db05-410b-9d0c-11ae0415e559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
